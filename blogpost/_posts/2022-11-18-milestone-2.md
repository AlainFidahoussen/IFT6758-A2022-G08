---
layout: post
title: Milestone 2
---

## Question 2 : Ingénierie des caractéristiques I 

1. Nous constatons que la très grande majorité des tirs (buts et non-buts) sont depuis la zone offensive (beaucoup de tirs à une distance plus petite que 89). La plupart des buts sont marqués à une courte distance du filet bien que certains soient également marqués à de loin (il serait alors intéressant de considérer lorsque les cages sont vides ou non). La tendance générale est que le nombre de tirs est inversement proportionnel à la distance. ![Image](../num_shots_goal_bin_distance.png)
De plus, les tireurs ont plus de chance de marquer lorsqu'ils sont face au filet, c'est à dire quand l'angle est de 0 (ce qui semble logique). En général, plus l'angle est grand, moins le joueur a tendance à tirer et moins il y a de buts. ![Image](../num_shots_goal_bin_angle.png) Enfin d'après le jointplot, les tirs sont concentrés à une distance courte et à un petit angle du but. Plus la distance est longue, plus les joueurs ont tendance à tirer en face du but. De même, plus l'angle est grand, plus la distance de tir est courte. ![Image](../num_shots_goal_jointplot.png)

2. La tendance que nous remarquons avec ce graphique est que le pourcentage de buts décroit avec la distance initialement, mais commence à remonter lorsque la distance augmente. Cela semble logique, car les chances qu'un tir soit un but sont plus grandes lorsque les tireurs sont proches du filet. Nous pouvons expliquer également les taux de buts très importants pour les distances importantes car les tireurs tirent peu de loin (à cause de la difficulté) et lorsque cela se réalise, cela veut dire qu'il y a de grandes chances de marquer (comme par exemple lorsque les buts sont vides). ![Image](../goal_vs_distance.png)Concernant les taux de buts em fonction de l'angle de tir, nous obtenons un comportement similaire à la distance, c'est-à-dire, plus l'angle est petit et plus la proportion de buts est grande.En effet, les taux de buts décroient avec la hausse de la valeur de l'angle. Mais tout comme avec la distance, nous observons une hausse du taux de buts pour les grandes valeurs d'angles qui peut être expliquée de la même manière que pour la distance: plus l'on se trouve sur le côté des buts et plus il est difficile de marquer (il y a donc peu de tirs qui se réalisent dans ces zones). Pour que ces tirs se réalisent, il y a donc de grandes chances pour que le tireur marque (encore une fois en prenant l'exemple de but vide).    ![Image](../goal_vs_angle.png)

3. Lorsque le gardien est présent, la grande majorités des buts sont marqués depuis la zone offensive. En revanche, nous constatons une augmentation du nombre de buts marqués à une distance très loin, depuis la zone défensive. En sachant que la zone défensive commence à X = -25 (rappel: nous avons standardisé les coordonnées en faisant une rotation de 180 degrés pour les tirs avec `rinkSide="left"`), en effectuant l'opération `df_goals[(df_goals['Is Empty'] == 0) & (df_goals['st_X'] < -25)]`, nous comptons 801 buts marqués depuis la zone défensive avec un gardien présent. Ces résultats étant étranges, nous pourrions alors nous demander si d'éventuelles erreurs ne sont pas présentes dans l'API.  ![Image](../empty_vs_noempty.png) Pour le prouver, prenons le jeu avec l'ID 2018020192, nous remarquons sur [cette vidéo](https://www.nhl.com/video/fla--wpg/t-277350912/c-62563403?q=2018020192) que l'équipe Winnipeg Jets, qui est l'équipe "home" se stiue à gauche de la patinoire pour la période 1. Par contre, dans la section liveData > linescore > periods > 0 > home > rinkSide de l'API du jeu, il est indiqué que cette équipe est à droite pour cette période. ![Image](../Wrong_rinkSide.png) Comme nous standardisons les coordonnées en fonction de l'information "rinkSide", si elle est erronée, alors les coordonnées standardisées ne seront pas bonnes non plus. Par exemple, à 01:39 de la vidéo, les Winnipeg Jets marque un but qui correspond à l'événement 59 de l'API. Les coordonnées de l'événement fournies par l'API sont (81, 5) ce qui est bon puisque le filet de l'équipe adverse est à droite. Toutefois,  l'information "rinkSide" étant inversée, les coordonnées standardisées sont (-81, -5) ce qui en fait un but marqué dans la zone défensive avec un gardien sur le terrain. ![Image](../Wrong_st_X.png)

## Question 3 : Modèles de base

Afin de prédire la probabilité qu'un tir soit un but (buts espérés), nous avons entraîné 
trois modèles de régression logistique, qui utilisent respectivement les caractéristiques suivantes : 
 - la distance au but 
 - l'angle de tir
 - la distance au but et l'angle de tir.
 
### Premiers résultats

 ```python
distance_data = features_data[['Shot distance', 'Is Goal']].dropna()
X = distance_data[['Shot distance']]
y = distance_data['Is Goal']

clf = LogisticRegression(random_state=RANDOM_SEED).fit(X_train, y_train)

accuracy = clf.score(X_valid, y_valid)
print(f'Accuracy on validation set = {np.around(100.*accuracy, 2)}%')

>>> Accuracy on validation set = 90.62%
```

À première vue, le modèle semble performant, car l'exactitude ("accuracy") obtenue est de 91%.
<a name="Imbalanced"></a>
Cependant, on remarque un fort débalancement des classes : 
![Image](../imbalanced_fig.png)

Il suffit dès lors de prédire systématiquement qu'un tir soit raté pour obtenir une performance similaire : 

 ```python
def dumb_model(X, y):
    pred = np.zeros_like(X)
    return (y == pred.ravel()).mean()


dumb_accuracy = dumb_model(X_valid, y_valid)
print(f'Dumb Accuracy on validation set = {np.around(100.*dumb_accuracy, 2)}%')

>>> Dumb Accuracy on validation set = 90.62%
```

La métrique utilisée ("accuracy") n'est pas adaptée lorsque les classes sont débalancées. Il faudrait 
plutôt évaluer le modèle en calculant le [f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html){:target="_blank"} ou le roc-auc. Par exemple : 

 ```python
f1Score = f1_score(y_valid, clf.predict(X_valid), average='macro')
print(f'F1-Score on validation set = {np.around(100.*f1Score, 2)}%')

rocauc = roc_auc_score(y_valid, clf.predict_proba(X_valid)[:,1])
print(f'ROC-AUC on validation set = {np.around(100.*rocauc, 2)}%')

>>> F1-Score on validation set = 47.54%
>>> ROC-AUC on validation set = 67.82%
```
### Graphiques de performance

Les 4 modèles que nous allons comparer sont : 
 - Une régression logistique appliquée à la distance au but.
 - Une régression logistique appliquée à l'angle de tir (en valeur absolue, pour des raisons de symmétrie).
 - Une régression logistique appliquée à la distance au but et à l'angle de tir.
 - Un modèle aléatoire uniforme. <br>

Voici un exemple de code utilisé. Le code complet se trouve dans le [notebook](https://github.com/AlainFidahoussen/IFT6758-A2022-G08/blob/57a581c0763aca4867bdef407dfedcda29b6495b/notebooks/Milestone-2/Q3_Modeles_Base.ipynb){:target="_blank"} 'notebooks/Milestone-2/Q3_Modeles_Base.ipynb'.
```python
RANDOM_SEED = 42

# Subset of the dataframe
distance_data = features_data[['Shot distance', 'Shot angle', 'Is Goal']]

# Features and target
X = distance_data[['Shot distance', 'Shot angle']]
y = distance_data['Is Goal']

# Stratify split
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)

# Logistic Regression
clf = LogisticRegression(random_state=RANDOM_SEED)
clf.fit(X_train, y_train)

y_pred = clf.predict_proba(X_valid)[:,1]
```
Nous souhaitons évaluer nos modèles à l'aide de 4 graphiques : 

 - <u>La courbe ROC</u> : <br>
La courbe ROC décrit le compromis entre le taux de faux positifs et le taux de vrais positifs.
La mesure AUC ("Area Under the Curve") représente l'aire sous la courbe ROC, et donne une mesure de performance du modèle. 
Le classifieur idéal a un AUC égal à 1.<br>
On constate dans le graphe ci-dessous (en haut à gauche) que les modèles de régression logistique performent un peu 
mieux qu'un modèle aléatoire uniforme, mais qu'aucun d'entre eux ne se détache. La meilleure performance est obtenue 
en prenant en compte à la fois la distance au but et l'angle de tir (AUC = 0.69) <br>

 - <u>La courbe de calibration</u> : <br>
Un modèle est dit bien calibré lorsque que la proportion d’événements ayant une même probabilité d’appartenir 
à une classe est égale à cette probabilité. 
On constate dans le graphe ci-dessous (en haut à droite) que les modèles de regréssion logistique sont bien 
calibrés, mais que les probabilités prédites sont faibles. On peut également voir que, comme attendu, le classifieur 
aléatoire uniforme est très mal calibré étant donné que la probabilité prédite est la même tout le temps.<br>

 - <u>Le taux de buts</u> : <br>
Un bon modèle aura des valeurs élevées de taux de buts pour des percentiles élevés. A contratio, un modèle 
aléatoire uniforme prédira toujours le même taux de buts.
On constate dans le graphe ci-dessous (en bas à gauche) qu'aucun des trois modèles de régression logistique n'est 
capable de prédire plus de 20% des buts, et ce, quelque soit la plage de performance du modèle considéré.<br>
 
  - <u>La proportion cumulée de buts</u> : <br>
Un bon modèle aura des valeurs élévées de proportions cumulées de buts pour des percentiles élevés, 
puis décroîtra rapidement (comportement inverse de la courbe de calibration). 
On constate dans le graphe ci-dessous (en bas à droite) que les modèles de régression logistique performent un peu 
mieux qu'un modèle aléatoire uniforme, mais qu'aucun d'entre eux ne se détache. Encore une fois, la meilleure 
performance semble être obtenue en prenant en compte à la fois la distance au but et l'angle de tir. <br>

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q3_BasisModels_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q3_BasisModels_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q3_BasisModels_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q3_BasisModels_Shot_Proba.png" width="400"/>
     </div>
</div>

### Expérimentations Comet

Voici les liens vers les expérimentations réalisées sur Comet, pour chacun des trois modèles :  

[Régression logistique avec la distance](https://www.comet.com/ift6758-a22-g08/logistic-regression/13ddd64727d24f0992c8de1fbed986cd?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall){:target="_blank"} <br>
[Régression logistique avec l'angle](https://www.comet.com/ift6758-a22-g08/logistic-regression/18029be53d7c493081d875638f8eb5d9?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall){:target="_blank"}  <br>
[Régression logistique avec la distance et l'angle](https://www.comet.com/ift6758-a22-g08/logistic-regression/22e4757c0af94157a7c238fa370e982e?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall){:target="_blank"} <br>

Par ailleurs, les trois modèles ont été enregistrés dans le [registre des modèles](https://www.comet.com/ift6758-a22-g08#model-registry){:target="_blank"}. <br>

## Question 4 : Ingénierie des caractéristiques II

### DataFrame des caractéristiques dans Comet

Voici le lien vers un extrait du dataframe (fichier csv) que nous allons utiliser par la suite pour entraîner nos modèles : 
[DataFrame GameID = 2017021065](https://www.comet.com/ift6758-a22-g08/feature-engineering-data/13a09db4522e4b488f9a8c8dc9520e16?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step){:target="_blank"} <br>
Ce dataframe représente les informations relatives au 
[match Winnipeg vs Washington du 12 mars 2018](https://www.nhl.com/gamecenter/wpg-vs-wsh/2018/03/12/2017021065){:target="_blank"}. <br>

Ce dataframe contient plusieurs caractéristiques qui seront filtrées par la suite, soit manuellement (car clairement 
non pertinentes), soit par des méthodes de sélection de caractéristiques. <br>

### Explication détaillée du dataframe

#### **Caractéristiques de bases**

- <u>Period seconds</u> (numérique) : temps écoulé, en secondes, depuis le début de la période courante. <br>
- <u>Game seconds</u> (numérique) : temps éoulé, en secondes,  depuis le début du match. Cette caractéristique est utile notamment pour suivre une pénalité 
qui se prolonge d'une période à une autre.<br>
- <u>X, Y</u> (numérique) : coordonnées de l'évènement
- <u>st_X, st_Y</u> (numérique) : coordonnées standardisées de l'événement : ce sont les coordonnées ramenées au coté droit de la patinoire. 
Ainsi, la cage est toujours située du côté positif (X=89, Y=0).
- <u>Shot distance</u> (numérique) : distance euclidienne entre les coordonnées standardisées du tir et les coordonnées de la cage.
- <u>Shot angle</u> (numérique) : angle de tir, en degrés. Un angle de 0 indique un tir effectué devant la cage, un angle de 90 indique un tir 
effectué à droite de la cage, et un angle de -90 à gauche de la cage.
- <u>Type de tir</u> (catégorique) : type de tir effectué par le joueur (exemple : Tip-In, Wrist Shot, Snap Shot etc.). 
- <u>Is Empty</u> (catégorique) : 1 si la cage est vide au moment du tir, 0 sinon. <br>

#### **Caractéristiques relatives au dernier événement**

- <u>Last event type</u> (catégorique) : type d'événement qui précède l'événement courant. 
Par souci de simplification, nous ignorons les événements de type 'Stoppage' (arrêt du gardien, 
resurfaçage de la patinoire etc.).<br>
- <u>Last event X, Last event Y</u> (numérique) : coordonnées du dernier événement. 
Comme précédemment, nous avons également calculé les coordonnées standardisées <u>Last event st_X</u> et <u>Last event st_Y</u>. <br>
- <u>Last event distance</u> (numérique) : distance euclidienne entre les coordonnées de l'événement courant et celles de 
l'événement précédent. <br>
- <u>Last event elapsed time</u> (numérique) : temps écoulé, en secondes, entre l'événement courant et le précédent. 
Par souci de simplification dans nos calculs, nous considérons que le temps minimum écoulé entre 
deux événements est de 0.5 secondes.<br>

#### **Caractéristiques avancées**
- <u>Rebound</u> (catégorique) : 1 si l'événement précédent est un tir, 0 sinon. <br>
- <u>Change in Shot Angle</u> (numérique) : changement d'angle de tir si l'événement est un rebond. <br>
- <u>Speed From Previous Event</u>  (numérique) : distance depuis l'événement précédent, divisée par le temps écoulé depuis l'événement précédent.<br>

#### **Caractéristiques bonus**
- <u>Strength</u> (catégorique) : Rapport de force de l'équipe qui prends le tir, comparé à l'équipe adverse : 
Even (même nombre de joueurs), Power Play (supériorité numérique) ou Short-Handed (infériorité numérique). <br>
- <u>Num players With</u> (catégorique ordinale) : nombre de joueurs présents sur la patinoire de l'équipe qui prend le tir (nombre de patineurs non-gardiens amicaux sur la glace). <br>
- <u>Num players Against</u> (catégorique ordinale) : nombre de joueurs présents sur la patinoire de l'équipe qui reçoit le tir (nombre de patineurs non-gardiens adverses sur la glace
). <br>
- <u>Elapsed time since Power Play</u> (numérique) : temps écoulé, en secondes, depuis une situation de Power Play.<br>

<a name="Q4_FeaturesAdd"></a>
#### **Caractéristiques additionnelles**
- <u>Shooter Goal Ratio Last Season</u> (numérique) : ratio du nombre de buts sur le nombre de tirs du tireur lors la saison précédente.<br>
- <u>Goalie Goal Ratio Last Season</u> (numérique) : ratio du nombre de buts encaissés sur le nombre de tirs subi par le gardien lors la saison précédente.<br>
- <u>Shooter Side</u> (catégorique) : indique si le joueur est droitier ('R') ou gaucher ('L').<br>
- <u>Shooter Ice Position</u> (catégorique) : indique la position officielle du joueur sur la patinoire : 'D' (Defenseman), 'C' (Center), 'L' (Left Wing), 
'R' (Right Wing) ou 'G' (Goalie).<br>

## Question 5 : Modèles avancés 
Nous procédons maintenant à l'entraînement de modèles avancés, en commençant par le modèle XGBoost. Les données (toutes les données sauf celles de 2019) ont été séparées en ensembles d'entraînement ("train") et de test. Voici les graphiques générés avec l'ensemble de test, avec une courbe pour chaque question.

 <div style="display:flex">
      <div style="flex:1;padding-right:10px;">
           <img src="figures/ROC_xgboost.png" width="400"/>
      </div>
      <div style="flex:1;padding-left:10px;">
           <img src="figures/Calibration_xgboost.png" width="400"/>
      </div>
 </div>
 <div style="display:flex">
      <div style="flex:1;padding-right:10px;">
           <img src="figures/Goal_Rate_xgboost.png" width="400"/>
      </div>
      <div style="flex:1;padding-left:10px;">
           <img src="figures/Shot_Proba_xgboost.png" width="400"/>
      </div>
 </div>

### Modèle XGBoost de base
Nous entraînons d'abord un modèle XGBoost de base (`XGBoost - Baseline`), avec tous les hyperparamètres défauts, et en utilisant seulement les caractéristiques de distance et d'angle.

Lorsqu'on compare avec les résultats pour le modèle de base de régression logistique, nous remarquons que la valeur AUC est plus grande ici (0.71 versus 0.68) ce qui indique une performance légèrement supérieure. En observant le taux de buts, nous remarquons aussi que le modèle XGBoost peut prédire au maximum environ 25% des buts, ce qui est aussi un peu meilleur que les modèles de régression logistique. En ce qui concerne la calibration, il semble que ce modèle peut prédire des probabilités plus hautes, mais que celles-ci sont mal calibrées.

### Optimisation d'hyperparamètres
Nous procédons ensuite à l'optimisation des hyperparamètres. Nous avons fait une recherche par grille pour 3 hyperparamètres: le taux d'apprentissage (learning rate), gamma (régularisation), et la profondeur maximale des arbres de décisions.

`<insert 3D heatmap grid>`

Les meilleurs hyperparamètres ont été déterminés en utilisant le score ROC-AUC du modèle entraîné avec chaque combinaison d'hyperparamètres, et ces valeurs sont: `{'gamma': 6, 'learning_rate': 0.1, 'max_depth': 8}`.

Comparé au modèle XGBoost de base, le modèle avec les hyperparamètres optimisés (`XGBoost - HPSearch`) paraît nettement supérieure. La valeur AUC est maintenant à 0.76, le taux de but prédit maximal a augmenté jusqu'à ~35%, et le modèle est en général assez bien calibré et capable de prédire des probabilités encore plus hautes.

### Sélection de caractéristiques
Enfin, nous avons testé différentes méthodes de sélection de caractéristiques (indépendemment de l'optimisation d'hyperparamètres) et leur impact sur la performance du modèle XGBoost.  Voici une courte description de chaque méthode:
- `SelectKBest`: Utilise (dans notre cas) le test de chi-carré entre chaque paire de caractéristiques pour calculer un score qui indique un degré d'indépendence pour chaque variable. Ensuite, on garde seulement les $k$ caractéristiques les plus indépendents. Nous avons fait une recherche d'hyperparamètres pour déterminer une bonne valeur pour $k$.
- Élimination récursive de caractéristiques avec `RFECV`: Le modèle de classification lui-même (donc XGBoost ici) fournit un poids d'importance à chaque caractéristique, et on continue à enlever des caractéristiques moins importantes jusqu'à ce qu'on atteind une performance optimale.
- Sélection séquentielle des caractéristiques avec `SequentialFeatureSelector`: En commençant avec zéro caractéristiques, à chaque itération on ajoute le caractéristique qui maximise le score de validation croisée, jusqu'à un certain nombre $k$ de caractéristiques. Nous avons aussi conduit une recherche d'hyperparamètres pour $k$.
- Sélection avec forêt aléatoire: Ici, on utilise le modèle de forêt aléatoire (Random Forest) pour identifier l'importance de chaque caractéristique, et à la fin on rejette celles où l'importance ne dépasse pas un certain seuil (par défaut, la moyenne).

`<bar chart - roc-auc score for each feature selection method>`
`<bar chart - f1 score for each feature selection method>`
Les figures précédentes illustrent les scores ROC-AUC et macro F1 d'entraînement calculé par validation croisée pour chaque méthode (sauf `SequentialFeatureSelector`). On voit que...

En utilisant la meilleure méthode de sélection (forêt aléatoire), on détermine que les caractéristiques optimales seraient ces 16: `['Period seconds', 'st_X', 'st_Y', 'Shot distance', 'Shot angle', 'Is Empty', 'Speed From Previous Event', 'Change in Shot Angle', 'Shooter Goal Ratio Last Season', 'Goalie Goal Ratio Last Season', 'Elapsed time since Power Play', 'Last event elapsed time', 'Last event st_X', 'Last event st_Y', 'Last event distance', 'Last event angle']`.

Lorsque nous faisons une comparaison entre le modèle XGBoost avec les hyperparamètres de défaut et utilisant la sélection par forêt aléatoire, le modèle de base, et le modèle avec hyperparamètres optimisés, il est clair que...

### Expériences sur Comet.ml

## Question 6 : Faites de votre mieux

#### Gestion des valeurs aberrantes
Comme il nous a été présenté en cours, plusieurs méthodes pour la gestion 
des valeurs aberrantes existent. 
Si dans un premier temps nous avons tenté de gérer ces valeurs au moyen 
de méthodes visuelles (boîtes à moustaches ou nuages de points), 
nous nous sommes rapidement tournés vers d'autres méthodes:
- IQR 
- Percentile (5%-95% ou 1%-99%)
- Z-score

Nous avons finalement décidé d'exclure les valeurs aberrantes en utilisant 
notre connaissance du domaine : il est extrêmement rare de marquer un  
but depuis la zone défensive alors que les cages ne sont pas vides :

```python
df.drop(df[
       (df['Is Goal'] == 1)  & 
       (df['Is Empty'] == 0) &
       (df['st_X'] < -25)
       ]
```


#### **Ingéniérie des caractéristiques**
Comme évoqué [précédemment](#Q4_FeaturesAdd), de nouvelles caractéristiques ont été ajoutées à nos modèles avec notamment :
 - Le ratio du nombre de buts sur le nombre de tirs lors de la saison précédente pour chaque 
tireur.
 - Le ratio du nombre de buts encaissés sur le nombre de tirs pour les gardiens de but.
 - La préférence manuelle des joueurs (droitier ou gaucher) 
 - La position des joueurs sur la patinoire (Défense, Centre, Aile gauche ou droite, ou gardien de but). <br>

L'objectif de ces ajouts est d'obtenir un maximum d'informations pour aider 
notre modèle lors de la prédiction en lui permettant ainsi d'avoir une "idée" de la qualité du 
tireur et si ce dernier se trouve dans une situation favorable pour marquer.
En plus de ceci, nous avons également effectué du regroupement (binning) pour les 
caractéristiques de distance et d'angle toujours dans l'optique 
d'améliorer les performances des modèles et de réduire le sur-apprentissage.

#### **Modèles testés**
En complément des modèles de regression logistique et XGBoost, de nouveaux modèles ont été testés. 
Notre approche au problème a été de tester un grand nombre de modèles issus de familles différentes 
afin de sélectionner les meilleurs d'entre eux. 
Ainsi voici une liste des modèles que nous avons pu utiliser: 
- Modèles utilisant les arbres de décision : [Adaboost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html){:target="_blank"}, 
[EasyEnsembleClassifier](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.EasyEnsembleClassifier.html){:target="_blank"} et 
[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html){:target="_blank"}.
- Méthodes des plus proches voisins : [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html){:target="_blank"} (K plus proches voisins).
- Réseau de neurones : [Multi Layer Perceptron](https://keras.io/guides/sequential_model/){:target="_blank"} (MLP).

#### **Sélection des caractéristiques**
A l'instar des modèles testés, nous avons implémenté diverses méthodes de sélection de caractéristiques pour nos modèles. 
Par conséquent, nous avons repris plusieurs méthodes vues en cours ainsi que celles proposées par la bibliothèque 
[ScikitLearn](https://scikit-learn.org/stable/modules/feature_selection.html){:target="_blank"}. 
Nous pouvons citer notamment:
- Méthodes de filtrage : Information mutuelle, Chi carré, Anova, Variance Threshold, Matrice de corrélation.
- Méthodes embarquées : Lasso (norme L1) réalisé à l'aide du modèle linéaire SVC, importance des caractéristiques basée sur les arbres aléatoires.
- Méthodes d'encapsulage : Recherche vers l'avant (forward) et arrière (backward) à partir du modèle SVC , RFE (recursive feature elimination) basée sur les arbres de décision.
- Réduction de dimensionnalité : Analyse des composantes principales (ACP)
- <font color='red'> SHAP? </font> <br>

La totalité de ces méthodes s'est avérée simple d'utilisation et rapide à exécuter à l'exception des méthodes "greedy" (recherches forward et backward) qui se sont révélées très chronophages (comme attendu). Pour cette raison ces dernières ont été moins utilisées.  

#### **Division des données d'entraînement**

Comme évoqué [précédemment](#Imbalanced), les données à notre disposition 
sont débalancées (90% des tirs sont des non-buts). 
Il est donc important de prendre en compte ce débalancement lors de la 
division de notre jeu de données en jeu de d'entraînement et de validation. 
Pour cela, l'option "stratify" (proposée par la librairie ScikitLearn) 
nous permet de conserver ces proportions pour chaque division que nous 
réalisons de notre jeu de données :

```python
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)
print(Counter(y_valid)[0] / Counter(y_valid)[1])
print(Counter(y)[0] / Counter(y)[1])

>> 9.660857441617289
>> 9.661229042490152
```

Cette méthode de division est utilisée pour entraîner et évaluer tous nos modèles. 



#### **Ajustement des hyperparamètres, stratégies de validation croisée**

Pour la recherche des meilleurs hyperparamètres, nous utilisons l'optimisation bayésienne qui permet 
de trouver un jeu d'hyperparamètres plus rapidement qu'une recherche de type "GridSearch".
Ces recherches d'hyperparamètres sont effectuées aussi bien pour les modèles 
que pour les méthodes de sélection de caractéristiques, notamment pour les méthodes de filtrage afin de savoir quel nombre de caractéristiques à garder était optimal. 

Concernant les modèles, les hyperparamètres évalués peuvent être :
 - La longueur, la profondeur et le coefficient gamma pour les modèles usant des arbres de décision.
 - Le nombre de couches et de neurones pour les réseaux de neurones.
 - Le nombre de voisins à considérer pour les modèles KNN. 
 
Concernant les techniques de sélection de caractéristiques, les hyperparamètres peuvent être : 
 - Le nombre de composantes pour l'analyse de composantes principales.
 - Le nombre de caractéristiques à conserver pour la méthode Anova.
 - Le terme de pénalité pour les méthodes Lasso.

L'évaluation des hyperparamètres est effectuée par des méthodes de validations croisées. 


#### **Régularisation**

Si l'utilisation de la norme L1 (LASSO) a été utilisée pour la sélection 
de caractéristiques comme vu auparavant, 
elle a également été utilisée pour la régularisation des modèles MLP. Pour ces derniers, la régularisation avec la norme L2 a aussi été implémentée ainsi qu'une combinaison de ces deux normes. 
L'élagage de nos modèles basés sur les arbres de décision a aussi été utilisé (controlé par le paramètres gamma vu dans la section précédente).

#### **Nouvelles métriques**

Nous avons pu voir que la métrique d'exactitude n'était pas un bon moyen d'évaluer les performances de notre modèle au vu du fort débalancement de nos données. Dès lors, nous avons choisi d'utiliser de nouvelles métriques notamment le score F1 macro ainsi que le ROC-AUC, qui au vu de nos recherches, ainsi que de nos connaissances, s'avèrent plus adaptées pour de telles données. 

#### **Observations et Résultats** 

La sélection des modèles s'est effectuée via une recherche d'hyperparamètres, couplée 
à des techniques de sélection de caractéristiques. Les modèles testés et les 
liens vers les expérimentations Comet correspondantes sont les suivants :  

 - Un Random Forest sur des caractéristiques sélectionnées manuellement : [Comet](https://www.comet.com/ift6758-a22-g08/hyperparameters-randomforest/view/new/experiments){:target="_blank"}
 - Un Random Forest sur des caractéristiques sélectionnées manuellement à l'aide de la méthode SHAP, avec un binning sur l'angle et la distance, et en excluant les caractéristiques à faible variance : [Comet](https://www.comet.com/ift6758-a22-g08/hyperparameters-randomforest-shap-binning-variancethreshold/view/new/experiments){:target="_blank"}
 - Un Adaboost en sélectionnant les caractéristiques via la méthode ANOVA : [Comet](https://www.comet.com/ift6758-a22-g08/hyperparameters-adaboost-anovaselector/view/new/experiments){:target="_blank"}
 - Un KNN en sélectionnant les caractéristiques via un SVM linéaire utilisant une pénalité L1 (Lasso) : [Comet](https://www.comet.com/ift6758-a22-g08/hyperparameters-knn-2/view/new/experiments){:target="_blank"}
 - Un EasyEnsemble en sélectionnant les caractéristiques via un PCA : [Comet](https://www.comet.com/ift6758-a22-g08/hyperparameters-easyensemble-pcaselector/view/new/experiments){:target="_blank"}

Cette recherche nous a permis de trouver, pour chacun des 5 modèles ci-dessus, les meilleurs hyperparamètres, 
puis d'entraîner les modèles avec ces derniers.
Les expérimentations correspondantes se trouvent [ici](https://www.comet.com/ift6758-a22-g08/best-models/view/new/panels){:target="_blank"}, 
et les modèles obtenus sont dans le [registre des modèles ](https://www.comet.com/ift6758-a22-g08#model-registry){:target="_blank"}. 

De manière générale, nous avons pu constater une légère hausse de nos performances lorsque 
nos utilisons des modèles basés sur des arbres de décision comme Random Forest ou Adaboost, chose à 
laquelle nous nous attendions car ces derniers se révèlent performants pour des données 
tabulaires. 

Nous avons également obtenu des résultats intéressants avec KNN. 
Mais il reste préférable d'utiliser des modèles basées les arbres car ces derniers sont 
plus performants mais également plus rapides à évaluer. 

Ajoutons également que le binning de nos caractéristiques d'angle et de distance a 
diminué les valeurs de nos métriques d'évaluation. En effet, nous avons pu comparer 
des modèles de Random Forest paramétrés identiquement mais ayant pour seule différence 
que l'un considère des données ayant subies un binning et l'autre des données non retouchées. 
La conséquence a été une légère baisse de nos valeurs de ROC-AUC ainsi que du score F1. 
On constate par ailleurs que le binning semble réduire le sur-apprentissage, car les métriques 
d'entraînement et de validation se retrouvent très proches.

Il est également intéressant d'observer que le modèle MLP se révèle mauvais dans cette 
tâche de prédiction de buts. 
En effet, les scores les plus faibles ont été obtenus avec ce modèle et ce malgré une multitude 
d'architectures testées, de diverses fonctions d'activation ainsi que de régularisation. 
Ajoutons le temps d'entraînement particulièrement important, comparé autres modèles et 
pour de piètres améliorations de nos métriques. 

Le tableau suivant résume les métriques obtenues pour les 5 modèles sélectionnées : 

|                                   | AUC Valid   |  F1 Valid 
|                                   | ----------- | ----------- |
|Random Forest                      |   0.78      |   0.63      |
|Random Forest - Variance Threshold |   0.76      |   0.62      |
|Adaboost - Anova                   |   0.76      |   0.63      |
|EasyEnsemble - PCA                 |   0.72      |   0.59      |
|KNN - Linear SVC Lasso             |   0.67      |   0.60      |

Finalement, comme on peut le voir dans le tableau ci-dessus, le meilleur modèle obtenu 
semble être un Random Forest avec une sélection des caractéristiques effectuées manuellement. 
Ceci est confirmé par les courbes [ci-dessous](#Best-Models-NonCalibrated).

À titre d'information, nous avons conservé les suivantes : 
 - Caractéristiques numériques : `Period seconds`, `st_X`, `st_Y`, `Shot distance`, `Shot angle`, 
`Speed From Previous Event`, `Change in Shot Angle`, `Shooter Goal Ratio Last Season`, 
`Goalie Goal Ratio Last Season`, `Elapsed time since Power Play`, `Last event elapsed time`, 
`Last event st_X`, `Last event st_Y`, `Last event distance`, `Last event angle`.
 - Caractéristiques ordinales : `Period`, `Num players With`, `Num players Against`, `Is Empty`, `Rebound`.
 - Caractéristiques nominales : `Shot Type`, `Strength`, `Shooter Side`, `Shooter Ice Position`.

<a name="Best-Models-NonCalibrated"></a>
#### **Graphiques des meilleurs modèles, non calibrés** 

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Shot_Proba.png" width="400"/>
     </div>
</div>


#### **Graphiques des meilleurs modèles, calibrés** 

Même s'il est possible d'obtenir un score de prédiction entre 0 et 1, les méthodes basées 
sur les arbres et les plus proches voisins ne sont pas des méthodes probabilistes à 
proprement parlé.
Par conséquent, comme constaté dans les courbes de calibration ci-dessus, ces méthodes 
sont mal calibrées, c'est-à-dire que la probabilité prédite ne reflète par la proportion 
réelle d'appartenir à la classe positive (probabilité de buts dans notre cas).

Nous avons donc recalibré les modèles obtenus, en utilisant [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn-calibration-calibratedclassifiercv){:target="_blank"}.

Les expérimentations correspondantes se trouvent [ici](https://www.comet.com/ift6758-a22-g08/best-models-calibrated/view/new/panels){:target="_blank"}, 
et les modèles obtenus sont dans le [registre des modèles ](https://www.comet.com/ift6758-a22-g08#model-registry){:target="_blank"}. 


<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_Shot_Proba.png" width="400"/>
     </div>
</div>



## Question 7 : Évaluer sur l'ensemble de test 

Nous utilisons ici les versions **non calibrées** des modèles.

### Saison 2019 - Regular

La distribution des données de la saison régulière 2019/2020 devrait logiquement être assez comparable à celle des autres saisons régulières sur lesquelles nous avons entrainé et validé nos modèles. 
Ainsi, pour chacun des modèles, nous nous attendons à obtenir des performances similaires à celles 
obtenues sur nos données de validation. 
Les figures ci-dessous nous confirment effectivement cette tendance. 
Par exemple, pour notre modèle de random forest, nous avons obtenu un F1 macro de 0.64 
sur cet ensemble, ce qui est similaire au score que nous avons eu sur 
l'ensemble de validation. La métrique AUC est également très similaire. 
Nous pouvons tirer les mêmes conclusion pour les trois modèles de régression logistique de base.

|                        | AUC Valid   | AUC Test    | F1 Valid  | F1 Test  |
|                        | ----------- | ----------- | --------- | -------- |
|Random Forest           |   0.76      |   0.79      |   0.63    |   0.64   |
|XGBoost                 |   0.76      |   0.77      |   0.58    |   0.55   |
|LogReg - Distance       |   0.68      |   0.70      |   0.48    |   0.47   |
|LogReg - Angle          |   0.57      |   0.55      |   0.48    |   0.47   |
|LogReg - Distance/Angle |   0.69      |   0.70      |   0.48    |   0.47   |

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Regular_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Regular_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Regular_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Regular_Shot_Proba.png" width="400"/>
     </div>
</div>

### Saison 2019 - Playoffs

Puisque ce sont seulement les 16 meilleures équipes de la saison régulière qui participent 
aux séries éliminatoires, nous pouvons nous attendre à ce que la distribution des données soit 
légèrement différente de la saison régulière. 
Par exemple, nous pouvons penser que les tireurs et les gardiens ont un meilleur ratio de but 
(`Shooter Goal Ration Last Season` et `Goalie Goal Ration Last Season`), 
que les tirs se font à une distance plus rapprochée du filet (`Shot Distance` plus courte) 
et qu'il est plus difficile de marquer (donc proportion moins grande de buts) 
étant donné une force plus similaire entre les équipes. 
Par ailleurs, il n'existe pas de période shootout en séries éliminatoires. 
Cette particularité de la saison playoff peut également modifier la distribution des données. 
Comme nos modèles ont été entrainés et validés uniquement sur les données des saisons régulières, 
ils sont moins généralisables sur la saison playoffs. 
C'est exactement ce que nous observons dans les figures ci-dessous. En effet, la courbe ROC par exemple 
suggère une performance qui peut-être moins bonne qu'un classifieur aléatoire.
À titre d'illustration, notre meilleur modèle de random forest a obtenu un F1 macro de 0.61 
sur cet ensemble contre 0.63 sur l'ensemble validation. 
En revanche, les performances des modèles de régression logistique de base restent 
stable à 0.48. Cela révèle probablement du sous-apprentissage. 
En effet, comme nous avons entrainé ces trois modèles de régression logistique sur 1 ou 2 caractéristiques 
seulement, ces modèles sont trop simples et vont produire des erreurs systématiques 
(biais élevé). 

|                        | AUC Valid   | AUC Test    | F1 Valid  | F1 Test  |
|                        | ----------- | ----------- | --------- | -------- |
|Random Forest           |   0.76      |   0.64      |   0.63    |   0.61   |
|XGBoost                 |   0.76      |   0.57      |   0.57   |   0.55   |
|LogReg - Distance       |   0.68      |   0.48      |   0.48    |   0.48   |
|LogReg - Angle          |   0.57      |   0.61      |   0.48    |   0.48   |
|LogReg - Distance/Angle |   0.69      |   0.49      |   0.48    |   0.48   |

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Playoffs_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Playoffs_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Playoffs_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Playoffs_Shot_Proba.png" width="400"/>
     </div>
</div>

