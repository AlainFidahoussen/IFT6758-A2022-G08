---
layout: post
title: Milestone 2
---

## Question 2 : Ingénierie des caractéristiques I 

1. Nous constatons que la très grande majorité des tirs (buts et non-buts) sont depuis la zone offensive (beaucoup de tirs à une distance plus petite que 89). La plupart des buts sont marqués à une courte distance du filet bien que certains soient également marqués à de loin (il serait alors intéressant de considérer lorsque les cages sont vides ou non). La tendance générale est que le nombre de tirs est inversement proportionnel à la distance. ![Image](../num_shots_goal_bin_distance.png)
De plus, les tireurs ont plus de chance de marquer lorsqu'ils sont face au filet, c'est à dire quand l'angle est de 0 (ce qui semble logique). En général, plus l'angle est grand, moins le joueur a tendance à tirer et moins il y a de buts. ![Image](../num_shots_goal_bin_angle.png) Enfin d'après le jointplot, les tirs sont concentrés à une distance courte et à un petit angle du but. Plus la distance est longue, plus les joueurs ont tendance à tirer en face du but. De même, plus l'angle est grand, plus la distance de tir est courte. ![Image](../num_shots_goal_jointplot.png)

2. La tendance que nous remarquons avec ce graphique est que le pourcentage de buts décroit avec la distance initialement, mais commence à remonter lorsque la distance augmente. Cela semble logique, car les chances qu'un tir soit un but sont plus grandes lorsque les tireurs sont proches du filet. Nous pouvons expliquer également les taux de buts très importants pour les distances importantes car les tireurs tirent peu de loin (à cause de la difficulté) et lorsque cela se réalise, cela veut dire qu'il y a de grandes chances de marquer (comme par exemple lorsque les buts sont vides). ![Image](../goal_vs_distance.png)Concernant les taux de buts em fonction de l'angle de tir, nous obtenons un comportement similaire à la distance, c'est-à-dire, plus l'angle est petit et plus la proportion de buts est grande.En effet, les taux de buts décroient avec la hausse de la valeur de l'angle. Mais tout comme avec la distance, nous observons une hausse du taux de buts pour les grandes valeurs d'angles qui peut être expliquée de la même manière que pour la distance: plus l'on se trouve sur le côté des buts et plus il est difficile de marquer (il y a donc peu de tirs qui se réalisent dans ces zones). Pour que ces tirs se réalisent, il y a donc de grandes chances pour que le tireur marque (encore une fois en prenant l'exemple de but vide).    ![Image](../goal_vs_angle.png)

3. Lorsque le gardien est présent, la grande majorités des buts sont marqués depuis la zone offensive. En revanche, nous constatons une augmentation du nombre de buts marqués à une distance très loin, depuis la zone défensive. En sachant que la zone défensive commence à X = -25 (rappel: nous avons standardisé les coordonnées en faisant une rotation de 180 degrés pour les tirs avec `rinkSide="left"`), en effectuant l'opération `df_goals[(df_goals['Is Empty'] == 0) & (df_goals['st_X'] < -25)]`, nous comptons 801 buts marqués depuis la zone défensive avec un gardien présent. Ces résultats étant étranges, nous pourrions alors nous demander si d'éventuelles erreurs ne sont pas présentes dans l'API.  ![Image](../empty_vs_noempty.png) Pour le prouver, prenons le jeu avec l'ID 2018020192, nous remarquons sur [cette vidéo](https://www.nhl.com/video/fla--wpg/t-277350912/c-62563403?q=2018020192) que l'équipe Winnipeg Jets, qui est l'équipe "home" se stiue à gauche de la patinoire pour la période 1. Par contre, dans la section liveData > linescore > periods > 0 > home > rinkSide de l'API du jeu, il est indiqué que cette équipe est à droite pour cette période. ![Image](../Wrong_rinkSide.png) Comme nous standardisons les coordonnées en fonction de l'information "rinkSide", si elle est erronée, alors les coordonnées standardisées ne seront pas bonnes non plus. Par exemple, à 01:39 de la vidéo, les Winnipeg Jets marque un but qui correspond à l'événement 59 de l'API. Les coordonnées de l'événement fournies par l'API sont (81, 5) ce qui est bon puisque le filet de l'équipe adverse est à droite. Toutefois,  l'information "rinkSide" étant inversée, les coordonnées standardisées sont (-81, -5) ce qui en fait un but marqué dans la zone défensive avec un gardien sur le terrain. ![Image](../Wrong_st_X.png)

## Question 3 : Modèles de base

Afin de prédire la probabilité qu'un tir soit un but (buts espérés), nous avons entraîné 
trois modèles de régression logistique, qui utilisent respectivement les caractéristiques suivantes : 
 - la distance au but 
 - l'angle de tir
 - la distance au but et l'angle de tir.
 
### Premiers résultats

 ```python
distance_data = features_data[['Shot distance', 'Is Goal']].dropna()
X = distance_data[['Shot distance']]
y = distance_data['Is Goal']

clf = LogisticRegression(random_state=RANDOM_SEED).fit(X_train, y_train)

accuracy = clf.score(X_valid, y_valid)
print(f'Accuracy on validation set = {np.around(100.*accuracy, 2)}%')

>>> Accuracy on validation set = 90.62%
```

À première vue, le modèle semble performant, car l'exactitude ("accuracy") obtenue est de 91%.
<a name="Imbalanced"></a>
Cependant, on remarque un fort débalancement des classes : 
![Image](../imbalanced_fig.png)

Il suffit dès lors de prédire systématiquement qu'un tir soit raté pour obtenir une performance similaire : 

 ```python
def dumb_model(X, y):
    pred = np.zeros_like(X)
    return (y == pred.ravel()).mean()


dumb_accuracy = dumb_model(X_valid, y_valid)
print(f'Dumb Accuracy on validation set = {np.around(100.*dumb_accuracy, 2)}%')

>>> Dumb Accuracy on validation set = 90.62%
```

La métrique utilisée ("accuracy") n'est pas adaptée lorsque les classes sont débalancées. Il faudrait 
plutôt évaluer le modèle en calculant le [f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html){:target="_blank"} ou le roc-auc. Par exemple : 

 ```python
f1Score = f1_score(y_valid, clf.predict(X_valid), average='macro')
print(f'F1-Score on validation set = {np.around(100.*f1Score, 2)}%')

rocauc = roc_auc_score(y_valid, clf.predict_proba(X_valid)[:,1])
print(f'ROC-AUC on validation set = {np.around(100.*rocauc, 2)}%')

>>> F1-Score on validation set = 47.54%
>>> ROC-AUC on validation set = 67.82%
```
### Graphiques de performance

Les 4 modèles que nous allons comparer sont : 
 - Une régression logistique appliquée à la distance au but.
 - Une régression logistique appliquée à l'angle de tir (en valeur absolue, pour des raisons de symmétrie).
 - Une régression logistique appliquée à la distance au but et à l'angle de tir.
 - Un modèle aléatoire uniforme. <br>

Voici un exemple de code utilisé. Le code complet se trouve dans le [notebook](https://github.com/AlainFidahoussen/IFT6758-A2022-G08/blob/57a581c0763aca4867bdef407dfedcda29b6495b/notebooks/Milestone-2/Q3_Modeles_Base.ipynb){:target="_blank"} 'notebooks/Milestone-2/Q3_Modeles_Base.ipynb'.
```python
RANDOM_SEED = 42

# Subset of the dataframe
distance_data = features_data[['Shot distance', 'Shot angle', 'Is Goal']]

# Features and target
X = distance_data[['Shot distance', 'Shot angle']]
y = distance_data['Is Goal']

# Stratify split
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)

# Logistic Regression
clf = LogisticRegression(random_state=RANDOM_SEED)
clf.fit(X_train, y_train)

y_pred = clf.predict_proba(X_valid)[:,1]
```
Nous souhaitons évaluer nos modèles à l'aide de 4 graphiques : 

 - <u>La courbe ROC</u> : <br>
La courbe ROC décrit le compromis entre le taux de faux positifs et le taux de vrais positifs.
La mesure AUC ("Area Under the Curve") représente l'aire sous la courbe ROC, et donne une mesure de performance du modèle. 
Le classifieur idéal a un AUC égal à 1.<br>
On constate dans le graphe ci-dessous (en haut à gauche) que les modèles de régression logistique performent un peu 
mieux qu'un modèle aléatoire uniforme, mais qu'aucun d'entre eux ne se détache. La meilleure performance est obtenue 
en prenant en compte à la fois la distance au but et l'angle de tir (AUC = 0.69) <br>

 - <u>La courbe de calibration</u> : <br>
Un modèle est dit bien calibré lorsque que la proportion d’événements ayant une même probabilité d’appartenir 
à une classe est égale à cette probabilité. 
On constate dans le graphe ci-dessous (en haut à droite) que les modèles de regréssion logistique sont bien 
calibrés, mais que les probabilités prédites sont faibles. On peut également voir que, comme attendu, le classifieur 
aléatoire uniforme est très mal calibré étant donné que la probabilité prédite est la même tout le temps.<br>

 - <u>Le taux de buts</u> : <br>
Un bon modèle aura des valeurs élevées de taux de buts pour des percentiles élevés. A contratio, un modèle 
aléatoire uniforme prédira toujours le même taux de buts.
On constate dans le graphe ci-dessous (en bas à gauche) qu'aucun des trois modèles de régression logistique n'est 
capable de prédire plus de 20% des buts, et ce, quelque soit la plage de performance du modèle considéré.<br>
 
  - <u>La proportion cumulée de buts</u> : <br>
Un bon modèle aura des valeurs élévées de proportions cumulées de buts pour des percentiles élevés, 
puis décroîtra rapidement (comportement inverse de la courbe de calibration). 
On constate dans le graphe ci-dessous (en bas à droite) que les modèles de régression logistique performent un peu 
mieux qu'un modèle aléatoire uniforme, mais qu'aucun d'entre eux ne se détache. Encore une fois, la meilleure 
performance semble être obtenue en prenant en compte à la fois la distance au but et l'angle de tir. <br>

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="ROC_Logistic_Regression_Distance_Angle.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="Calibration_Logistic_Regression_Distance_Angle.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="Goal_Rate_Logistic_Regression_Distance_Angle.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="Shot_Proba_Logistic_Regression_Distance_Angle.png" width="400"/>
     </div>
</div>

### Expérimentations Comet

Voici les liens vers les expérimentations réalisées sur Comet, pour chacun des trois modèles :  

[Régression logistique avec la distance](https://www.comet.com/ift6758-a22-g08/logistic-regression/13ddd64727d24f0992c8de1fbed986cd?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall){:target="_blank"} <br>
[Régression logistique avec l'angle](https://www.comet.com/ift6758-a22-g08/logistic-regression/18029be53d7c493081d875638f8eb5d9?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall){:target="_blank"}  <br>
[Régression logistique avec la distance et l'angle](https://www.comet.com/ift6758-a22-g08/logistic-regression/22e4757c0af94157a7c238fa370e982e?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=wall){:target="_blank"} <br>

Par ailleurs, les trois modèles ont été enregistrés dans le [registre des modèles](https://www.comet.com/ift6758-a22-g08#model-registry){:target="_blank"}. <br>

## Question 4 : Ingénierie des caractéristiques II

### DataFrame des caractéristiques dans Comet

Voici le lien vers un extrait du dataframe (fichier csv) que nous allons utiliser par la suite pour entraîner nos modèles : 
[DataFrame GameID = 2017021065](https://www.comet.com/ift6758-a22-g08/feature-engineering-data/13a09db4522e4b488f9a8c8dc9520e16?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step){:target="_blank"} <br>
Ce dataframe représente les informations relatives au 
[match Winnipeg vs Washington du 12 mars 2018](https://www.nhl.com/gamecenter/wpg-vs-wsh/2018/03/12/2017021065){:target="_blank"}. <br>

Ce dataframe contient plusieurs caractéristiques qui seront filtrées par la suite, soit manuellement (car clairement 
non pertinentes), soit par des méthodes de sélection de caractéristiques. <br>

### Explication détaillée du dataframe

#### **Caractéristiques de bases**

- <u>Period seconds</u> (numérique) : temps écoulé, en secondes, depuis le début de la période courante. <br>
- <u>Game seconds</u> (numérique) : temps éoulé, en secondes,  depuis le début du match. Cette caractéristique est utile notamment pour suivre une pénalité 
qui se prolonge d'une période à une autre.<br>
- <u>X, Y</u> (numérique) : coordonnées de l'évènement
- <u>st_X, st_Y</u> (numérique) : coordonnées standardisées de l'événement : ce sont les coordonnées ramenées au coté droit de la patinoire. 
Ainsi, la cage est toujours située du côté positif (X=89, Y=0).
- <u>Shot distance</u> (numérique) : distance euclidienne entre les coordonnées standardisées du tir et les coordonnées de la cage.
- <u>Shot angle</u> (numérique) : angle de tir, en degrés. Un angle de 0 indique un tir effectué devant la cage, un angle de 90 indique un tir 
effectué à droite de la cage, et un angle de -90 à gauche de la cage.
- <u>Type de tir</u> (catégorique) : type de tir effectué par le joueur (exemple : Tip-In, Wrist Shot, Snap Shot etc.). 
- <u>Is Empty</u> (catégorique) : 1 si la cage est vide au moment du tir, 0 sinon. <br>

#### **Caractéristiques relatives au dernier événement**

- <u>Last event type</u> (catégorique) : type d'événement qui précède l'événement courant. 
Par souci de simplification, nous ignorons les événements de type 'Stoppage' (arrêt du gardien, 
resurfaçage de la patinoire etc.).<br>
- <u>Last event X, Last event Y</u> (numérique) : coordonnées du dernier événement. 
Comme précédemment, nous avons également calculé les coordonnées standardisées <u>Last event st_X</u> et <u>Last event st_Y</u>. <br>
- <u>Last event distance</u> (numérique) : distance euclidienne entre les coordonnées de l'événement courant et celles de 
l'événement précédent. <br>
- <u>Last event elapsed time</u> (numérique) : temps écoulé, en secondes, entre l'événement courant et le précédent. 
Par souci de simplification dans nos calculs, nous considérons que le temps minimum écoulé entre 
deux événements est de 0.5 secondes.<br>

#### **Caractéristiques avancées**
- <u>Rebound</u> (catégorique) : 1 si l'événement précédent est un tir, 0 sinon. <br>
- <u>Change in Shot Angle</u> (numérique) : changement d'angle de tir si l'événement est un rebond. <br>
- <u>Speed From Previous Event</u>  (numérique) : distance depuis l'événement précédent, divisée par le temps écoulé depuis l'événement précédent.<br>

#### **Caractéristiques bonus**
- <u>Strength</u> (catégorique) : Rapport de force de l'équipe qui prends le tir, comparé à l'équipe adverse : 
Even (même nombre de joueurs), Power Play (supériorité numérique) ou Short-Handed (infériorité numérique). <br>
- <u>Num players With</u> (catégorique ordinale) : nombre de joueurs présents sur la patinoire de l'équipe qui prend le tir (nombre de patineurs non-gardiens amicaux sur la glace). <br>
- <u>Num players Against</u> (catégorique ordinale) : nombre de joueurs présents sur la patinoire de l'équipe qui reçoit le tir (nombre de patineurs non-gardiens adverses sur la glace
). <br>
- <u>Elapsed time since Power Play</u> (numérique) : temps écoulé, en secondes, depuis une situation de Power Play.<br>

<a name="Q4_FeaturesAdd"></a>
#### **Caractéristiques additionnelles**
- <u>Shooter Goal Ratio Last Season</u> (numérique) : ratio du nombre de buts sur le nombre de tirs du tireur lors la saison précédente.<br>
- <u>Goalie Goal Ratio Last Season</u> (numérique) : ratio du nombre de buts encaissés sur le nombre de tirs subi par le gardien lors la saison précédente.<br>
- <u>Shooter Side</u> (catégorique) : indique si le joueur est droitier ('R') ou gaucher ('L').<br>
- <u>Shooter Ice Position</u> (catégorique) : indique la position officielle du joueur sur la patinoire : 'D' (Defenseman), 'C' (Center), 'L' (Left Wing), 
'R' (Right Wing) ou 'G' (Goalie).<br>

## Question 5 : Modèles avancés 

## Question 6 : Faites de votre mieux

#### Gestion des valeurs aberrantes
Comme il nous a été présenté en cours, plusieurs méthodes pour la gestion des valeurs aberrantes existent. Si dans un premier temps nous avons tenté de gérer ces valeurs au moyen de méthodes visuelles (boîtes à moustaches ou nuages de points), nous nous sommes rapidement tournés vers d'autres méthodes:
- IQR 
- Percentile (5%-95% ou 1%-99%)
- Z-score

Ces méthodes ont notamment été utilisées pour notre caractéristique "st_X" (la coordonnée standardisée de X). A la suite de ce projet et pour nos recherches, nous avons utilisé la méthode Percentile afin d'écarter ces valeurs aberrantes pour tous nos modèles.

#### **Ingéniérie des caractéristiques**
Comme évoqué [précédemment](#Q4_FeaturesAdd), de nouvelles caractéristiques ont été ajoutées à nos modèles avec notamment :
 - Le ratio du nombre de buts sur le nombre de tirs lors de la saison précédente pour chaque 
tireur.
 - Le ratio du nombre de buts encaissés sur le nombre de tirs pour les gardiens de but.
 - La préférence manuelle des joueurs (droitier ou gaucher) 
 - La position des joueurs sur la patinoire (Défense, Centre, Aile gauche ou droite, ou gardien de but). <br>

L'objectif de ces ajouts est d'obtenir un maximum d'informations pour aider 
notre modèle lors de la prédiction en lui permettant ainsi d'avoir une "idée" de la qualité du 
tireur et si ce dernier se trouve dans une situation favorable pour marquer.
En plus de ceci, nous avons également effectué du regroupement (binning) pour les 
caractéristiques de distance et d'angle toujours dans l'optique 
d'améliorer les performances des modèles et de réduire le sur-apprentissage.

#### **Modèles testés**
En complément des modèles de regression logistique et XGBoost, de nouveaux modèles ont été testés. 
Notre approche au problème a été de tester un grand nombre de modèles issus de familles différentes 
afin de sélectionner les meilleurs d'entre eux. 
Ainsi voici une liste des modèles que nous avons pu utiliser: 
- Modèles utilisant les arbres de décision : [Adaboost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html){:target="_blank"}, 
[EasyEnsembleClassifier](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.EasyEnsembleClassifier.html){:target="_blank"} et 
[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html){:target="_blank"}.
- Méthodes des plus proches voisins : [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html){:target="_blank"} (K plus proches voisins).
- Réseau de neurones : [Multi Layer Perceptron](https://keras.io/guides/sequential_model/){:target="_blank"} (MLP).

#### **Sélection des caractéristiques**
A l'instar des modèles testés, nous souhaitons implémenter diverses méthodes de sélection de caractéristiques pour nos modèles. Par conséquent, nous avons repris plusieurs méthodes vues en cours ainsi que celles proposées par la bibliothèque [ScikitLearn](https://scikit-learn.org/stable/modules/feature_selection.html){:target="_blank"}. 
Nous pouvons citer notamment:
- Méthodes de filtrage : Information mutuelle, Chi carré, Anova, Variance Threshold, Matrice de corrélation.
- Méthodes embarquées : Lasso (norme L1) réalisé à l'aide du modèle linéaire SVC, importance des caractéristiques basée sur les arbres aléatoires.
- Méthodes d'encapsulage : Recherche vers l'avant (forward) et arrière (backward) à partir du modèle SVC , RFE (recursive feature elimination) basée sur les arbres de décision.
- Réduction de dimensionnalité : Analyse des composantes principales (ACP)
- <font color='red'> SHAP? </font> <br>

La totalité de ces méthodes s'est avérée simple d'utilisation et rapide à exécuter à l'exception des méthodes "greedy" (recherches forward et backward) qui se sont révélées très chronophages (comme attendu). Pour cette raison ces dernières ont été moins utilisées.  

#### **Division des données d'entraînement**

Comme évoqué [précédemment](#Imbalanced), les données à notre disposition 
sont débalancées (90% des tirs sont des non-buts). 
Il est donc important de prendre en compte ce débalancement lors de la 
division de notre jeu de données en jeu de d'entraînement et de validation. 
Pour cela, l'option "stratify" (proposée par la librairie ScikitLearn) 
nous permet de conserver ces proportions pour chaque division que nous 
réalisons de notre jeu de données :

```python
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)
print(Counter(y_valid)[0] / Counter(y_valid)[1])
print(Counter(y)[0] / Counter(y)[1])

>> 9.660857441617289
>> 9.661229042490152
```

Cette méthode de division est utilisée pour entraîner et évaluer tous nos modèles. 



#### **Ajustement des hyperparamètres, stratégies de validation croisée**

Pour la recherche des meilleurs hyperparamètres, nous utilisons l'optimisation bayésienne qui permet 
de trouver un jeu d'hyperparamètres plus rapidement qu'une recherche de type "GridSearch".
Ces recherches d'hyperparamètres sont effectuées aussi bien pour les modèles 
que pour les méthodes de sélection de caractéristiques, notamment pour les méthodes de filtrage afin de savoir quel nombre de caractéristiques à garder était optimal. 

Concernant les modèles, les hyperparamètres évalués peuvent être :
 - La longueur, la profondeur et le coefficient gamma pour les modèles usant des arbres de décision.
 - Le nombre de couches et de neurones pour les réseaux de neurones.
 - Le nombre de voisins à considérer pour les modèles KNN. 
 
Concernant les techniques de sélection de caractéristiques, les hyperparamètres peuvent être : 
 - Le nombre de composantes pour l'analyse de composantes principales.
 - Le nombre de caractéristiques à conserver pour la méthode Anova.
 - Le terme de pénalité pour les méthodes Lasso.

L'évaluation des hyperparamètres est effectuée par des méthodes de validations croisées. 


#### **Régularisation**

Si l'utilisation de la norme L1 (LASSO) a été utilisée pour la sélection 
de caractéristiques comme vu auparavant, 
elle a également été utilisée pour la régularisation des modèles MLP. Pour ces derniers, la régularisation avec la norme L2 a aussi été implémentée ainsi qu'une combinaison de ces deux normes. 
L'élagage de nos modèles basés sur les arbres de décision a aussi été utilisé (controlé par le paramètres gamma vu dans la section précédente).

#### **Nouvelles métriques**

Nous avons pu voir que la métrique d'exactitude n'était pas un bon moyen d'évaluer les performances de notre modèle au vu du fort débalancement de nos données. Dès lors, nous avons choisi d'utiliser de nouvelles métriques notamment le score F1 macro ainsi que le ROC-AUC, qui au vu de nos recherches, ainsi que de nos connaissances, s'avèrent plus adaptées pour de telles données. 

#### **Observations et Résultats** 

Nos recherches nous ont permis de récupérer plusieurs modèles dont les performances se distinguent des autres expériences menées (ces meilleures expériences sont disponibles sur [Comet](https://www.comet.com/ift6758-a22-g08/best-models/view/new/experiments){:target="_blank"}). De manière générale, nous avons pu constater une légère hausse de nos performances lorsque nos utilisons des modèles basés sur des arbres de décision comme Random Forest ou Adaboost (chose à laquelle nous nous attendions car ces derniers se révèlent performants pour des données tabulaires). Néanmoins, nous avons pu obtenir des résultats relativement proches de ces derniers modèles avec KNN. Mais il reste préférable d'utiliser des modèles utilisant les arbres car ceux la restent tout de même plus performants mais également plus rapides à entraîner. 
Ajoutons également que le binning de nos caractéristiques d'angle et de distance a diminué les valeurs de nos métriques d'évaluation. En effet, nous avons pu comparer des modèles de Random Forest paramétrés identiquement mais ayant pour seule différence que l'un considère des données ayant subies un binning et l'autre des données non retouchées. La conséquence a été une légère baisse de nos valeurs de ROC-AUC ainsi que du score F1. 
Il est également intéressant d'observer que le modèle MLP se révèle mauvais dans cette tâche de prédiction de buts. En effet, les scores les plus faibles ont été obtenus avec ce modèle et ce malgré une multitude d'architectures testées, de diverses fonctions d'activation ainsi que de régularisation. Ajoutons le temps d'entraînement particulièrement important face aux autres modèles et pour de piètres améliorations de nos métriques. 
Finalement, une multitude de modèles combinée à de nombreuses sélections de caractéristiques et de combinaisons d'hyperparamètres a été testée et à titre comparatif, un modèle Random Forest sans aucune sélection de caractéristiques a alors été implémenté. De manière surprenante, ce modèle s'est avéré être le plus performant de tous.



#### **Graphiques des meilleurs modèles, non calibrés** 
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Shot_Proba.png" width="400"/>
     </div>
</div>


#### **Graphiques des meilleurs modèles, calibrés** 
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q6_Best_Models_Calibrated_Shot_Proba.png" width="400"/>
     </div>
</div>





## Question 7 : Évaluer sur l'ensemble de test 

### Saison 2019 - Regular

La distribution des données de la saison régulière 2019/2020 devrait logiquement être assez comparable à celle des autres saisons régulières sur lesquelles nous avons entrainé et validé nos modèles. Ainsi, nous nous attendons à des performances semblables de nos modèles sur cet ensemble de test en comparaison de ceux réalisés sur l'ensemble de validation. Nous voyons effectivement cette tendance. Par exemple, pour notre modèle de Random Forest, nous avons obtenu un score F1 macro de 0.64 sur cet ensemble, ce qui est identique au score que nous avons eu sur l'ensemble de validation. Le score ROC-AUC est aussi très similaire. Nous pouvons tirer les mêmes conclusions pour les trois modèles de régression logistique.

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Regular_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Regular_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Regular_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Regular_Shot_Proba.png" width="400"/>
     </div>
</div>

### Saison 2019 - Playoffs

Puisque ce sont seulement les 16 meilleures équipes de la saison régulière qui participent aux séries éliminatoires, nous pouvons nous attendre à ce que la distribution des données de cette saison soit légèrement différente de la saison régulière. Par exemple, nous pouvons penser que les tireurs et les gardiens ont un meilleur ratio de but (`Shooter Goal Ration Last Season` et `Goalie Goal Ration Last Season`), que les tirs se font à une distance plus rapprochée du filet (`Shot Distance` plus courte) et qu'il est plus difficile de marquer (donc proportion moins grande de buts) étant donné une force plus similaire entre les équipes. Par ailleurs, il n'existe pas de période shootout en séries éliminatoires. Cette particularité de la saison playoff peut également modifier la distribution des données. Comme nos modèles ont été entrainés et validés uniquement sur les données des saisons régulières, ils sont moins généralisables sur la saison playoffs. C'est exactement ce que nous observons. À titre d'illustration, notre meilleur modèle de random forest a obtenu un F1 macro de 0.61 sur cet ensemble contre 0.64 sur l'ensemble validation. En revanche, les performances des modèles de régression logistique de base restent stable à 0.48. Cela révèle probablement du sous-apprentissage. En effet, comme nous avons entrainé ces trois modèles de régression logistique sur 1 ou 2 caractéristiques seulement, ces modèles sont trop simples et vont produire des erreurs systématiques (biais élevé). 

<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Playoffs_ROC.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Playoffs_Calibration.png" width="400"/>
     </div>
</div>
<div style="display:flex">
     <div style="flex:1;padding-right:10px;">
          <img src="./figures/Q7_Playoffs_Goal_Rate.png" width="400"/>
     </div>
     <div style="flex:1;padding-left:10px;">
          <img src="./figures/Q7_Playoffs_Shot_Proba.png" width="400"/>
     </div>
</div>

